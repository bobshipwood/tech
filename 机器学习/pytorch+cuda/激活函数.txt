隐藏层：

优先使用ReLU（或改进版如Leaky ReLU、Parametric ReLU）。

避免使用Sigmoid（梯度消失问题严重）。

输出层：

二分类任务：输出层用Sigmoid（配合交叉熵损失函数）。

多分类任务：输出层用Softmax。

回归任务：输出层可不使用激活函数（或使用线性函数）。